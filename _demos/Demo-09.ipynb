{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimhaines37/DataScience/blob/main/_demos/Demo-09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z33ZyHiPeOD8"
      },
      "source": [
        "# Demo 09 - Testing and Evaluating Models\n",
        "\n",
        "In this notebook we explore how to evaluate regression models and perform cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHJdbIbteOEG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/cmps3160\n",
        "# !git pull\n",
        "%cd _demos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVsJbLT-eOEK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import zscore\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "matplotlib.style.use('fivethirtyeight')\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKAGWP6cfKhy"
      },
      "source": [
        "## Loading and Understanding the Data\n",
        "\n",
        "For this work we will use the same data from the previous demo: [\"Generalized body composition prediction equation for men using simple measurement techniques\"](http://staff.pubhealth.ku.dk/~tag/Teaching/share/data/Bodyfat.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5IyUDbBfR4G"
      },
      "outputs": [],
      "source": [
        "# Load the Penrose Data\n",
        "# Select a subste of columns\n",
        "# Z-score all columns\n",
        "# Make pairplot\n",
        "input_names = ['Age', 'Neck', 'Forearm', 'Wrist']\n",
        "output_name = 'bodyfat'\n",
        "df = pd.read_csv(\"./data/bodyfat.csv\")[input_names + [output_name]].apply(zscore)\n",
        "sns.pairplot(df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUA260w1gBVs"
      },
      "source": [
        "## Linear Regression\n",
        "Scikit-learn implements [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) as well.\n",
        "\n",
        "All regression/classification classes in scikit-learn assume two functions:\n",
        "\n",
        "- `.fit(X, y)`: fit/train the model giving training data `X` (the feature matrix) and `y` (the true values)\n",
        "- `.predict(X)`: given a feature matrix, return the predicted `y` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph676NkTgjqG"
      },
      "outputs": [],
      "source": [
        "# Let's set the X and y variables.\n",
        "X = df[input_names]\n",
        "y = df[output_name]\n",
        "display(X.head())\n",
        "display(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Ej1p1HgEgj"
      },
      "outputs": [],
      "source": [
        "# Create a new regression model\n",
        "lr = LinearRegression()\n",
        "# fit the model on all data\n",
        "lr.fit(X,y)\n",
        "# predict on all data\n",
        "y_predicted = lr.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m576EVXhqP6"
      },
      "source": [
        "## Mean Average Error\n",
        "\n",
        "How good is this model? To start, let's use Mean Average Error for evaluation:\n",
        "\n",
        "$$MAE = \\frac{1}{n}\\sum_i^n |y - y_{\\text{predicted}}|$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhP4VfnfiCG3"
      },
      "outputs": [],
      "source": [
        "def mae(y, y_predicted):\n",
        "  return (y - y_predicted).abs().mean()\n",
        "\n",
        "mae(y, y_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8lmfxRiLED"
      },
      "source": [
        "### Interpreting MAE\n",
        "\n",
        "Since we have already z-scored the $y$ variables, MAE$=1$ means that, on average, our prediction is one standard-deviation from the truth. That would not be very good for most applications.\n",
        "\n",
        "We can also make a scatter plot comparing the true values with predicted values.\n",
        "\n",
        "**Question:** What does the set of \"perfect predictions\" look like on this graph?\n",
        "\n",
        "**Question:** What type of error is this? What did we do \"wrong\" from last lecture?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-PwiZXehW51"
      },
      "outputs": [],
      "source": [
        "# plot truth vs predicted\n",
        "def plot_predictions(y, y_predicted):\n",
        "  plt.figure(figsize=(6,5))\n",
        "  plt.scatter(y_predicted, y)\n",
        "  plt.xlabel('predicted y')\n",
        "  plt.ylabel('true y')\n",
        "  plt.title('MAE=%.3f' % mae(y, y_predicted))\n",
        "  # make x/y ranges the same\n",
        "  min_val = min(min(y), min(y_predicted))-.2\n",
        "  max_val = max(max(y), max(y_predicted))+.2\n",
        "  plt.xlim(min_val, max_val)\n",
        "  plt.ylim(min_val, max_val)\n",
        "  # plot line for \"perfect predictions\"\n",
        "  plt.plot([min_val, max_val], [min_val, max_val], 'k-', lw=1, alpha=.5)\n",
        "  plt.show() \n",
        "\n",
        "plot_predictions(y, y_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thjYO5zGk9Ga"
      },
      "source": [
        "## Train/Test Splits\n",
        "\n",
        "The MAE above is calculated on the training data. As we discussed, for prediction purposes, we don't really care much about the error on the training data. Instead, we care what the expected error will be on **new, unseen** data. This is what we call **generalization error**.\n",
        "\n",
        "Splitting the data into a train and test set is a simple way to simulate what our error would be on a new, unseen sample of data.\n",
        "\n",
        "Below, we'll split our data randomly into equal-sized train/test sets, then calculate error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNSQYL-TlWk7"
      },
      "outputs": [],
      "source": [
        "def split_data(X, y, training_fraction=.5):\n",
        "  # sample the training and testing indices \n",
        "  # setting random_state fixes the seed of the random number generator so we\n",
        "  # get the same split each time (otherwise it can be very hard to reproduce/debug results!)\n",
        "  train_idx = X.sample(frac=training_fraction, random_state=42).index\n",
        "  test_idx = X.index[~X.index.isin(train_idx)]\n",
        "  return (X.iloc[train_idx], y.iloc[train_idx], X.iloc[test_idx], y.iloc[test_idx])\n",
        "\n",
        "X_train, y_train, X_test, y_test = split_data(X, y, training_fraction=0.5)\n",
        "print('%d training and %d testing samples' % (len(X_train), len(X_test)))\n",
        "print('first training instance: ')\n",
        "display(X_train.head(1)) # this will change if I change random_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuUWgEtSm-Sn"
      },
      "source": [
        "### Test error\n",
        "\n",
        "Now, we can train on the training data and test on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqVBZtmOnM1M"
      },
      "outputs": [],
      "source": [
        "lr.fit(X_train,y_train)\n",
        "y_predicted = lr.predict(X_test)\n",
        "plot_predictions(y_test, y_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yehASBmrnaGb"
      },
      "source": [
        "**Question: How does this error compare to the training error above?**\n",
        "\n",
        "**Question:** This only gives us one measure of error? What if we got lucky or unlucky? How could we get more data for testing?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azM8J8TJoAyL"
      },
      "source": [
        "\n",
        "## Cross-validation\n",
        "But, this is just one split of the data. Maybe we got lucky/unlucky? \n",
        "\n",
        "If we really care about estimating generalization error, we can repeat this splitting process many times to get a more robust and reliable estimate. The process we use is **cross-validation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV8BUAdshXb7"
      },
      "source": [
        "![](https://github.com/nmattei/cmps3160/blob/master/_labs/images/k-folds.png?raw=1)\n",
        "\n",
        "\n",
        "We implement cross-validation ourselves below, but you can also just use the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) implementation in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKY8qlqfnMTT"
      },
      "outputs": [],
      "source": [
        "def cross_validation(X, y, n_folds=5):\n",
        "  \"\"\"\n",
        "  Return an iterator over (training index, testing index) pairs\n",
        "  for each cross-validation fold.\n",
        "  \"\"\"\n",
        "  # test size -- we might lose a few at the end here if there is a remainder.\n",
        "  test_size = int(len(y)/n_folds) \n",
        "  # all possible indices\n",
        "  all_idx = set(np.arange(len(y)))\n",
        "  for i in range(n_folds):\n",
        "    # slides the test window forward\n",
        "    test_idx = np.arange(i*test_size, i*test_size+test_size)\n",
        "    # use the rest as training indices.\n",
        "    train_idx = np.array(list(all_idx - set(test_idx)))\n",
        "    # yield vs return: this makes the method Iterable\n",
        "    yield (train_idx, test_idx)\n",
        "\n",
        "# let's test on a simple example of 5-fold cross validation on 10 instances\n",
        "X_small = X.head(10)  # what happens if we use 11 instances?\n",
        "y_small = y.head(10)\n",
        "# because we use yield, we can iterate over the output\n",
        "# of the cross-validation function.\n",
        "for train_idx, test_idx in cross_validation(X_small, y_small):\n",
        "  print('train_idx=%s  test_idx=%s' % ((train_idx, test_idx)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_rPgIptlWEA"
      },
      "outputs": [],
      "source": [
        "# Now, let's compute the MAE for 10 folds of cross-validation on the full dataset.\n",
        "maes = []\n",
        "for train_idx, test_idx in cross_validation(X, y, n_folds=10):\n",
        "  lr.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "  maes.append(mae(y.iloc[test_idx], lr.predict(X.iloc[test_idx])))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(maes, 'o')\n",
        "plt.xlabel('fold number')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()\n",
        "\n",
        "display(maes)\n",
        "print('mean MAE=%.3f, std=%.3f' % (np.mean(maes), np.std(maes)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiCpJGiU-hCG"
      },
      "source": [
        "That's a lot of variation!!\n",
        "\n",
        "This is not too surprising. Since we have less than 300 examples, changing the training set by 30 instances in each fold is more than a 10% change. Thus, we get very different models and very different test errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nqco47fBIwe"
      },
      "source": [
        "## Baselines\n",
        "\n",
        "So, how do we know if the MAE above is any good? To put such numbers in context, it is often helpful to have a simple baseline to compare to. What is a simple baseline for this task?\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "Well, we could just return a random number sampled uniformly from the range of $y$ values seen in the training set.\n",
        "\n",
        "Is there something slightly better that is also easy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEZ3vMSlBzm3"
      },
      "outputs": [],
      "source": [
        "# First, let's make a function to compute the mean error and stddev\n",
        "def cv_error(X, y, model):\n",
        "  maes = []\n",
        "  for train_idx, test_idx in cross_validation(X, y, n_folds=10):\n",
        "    model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "    maes.append(mae(y.iloc[test_idx], model.predict(X.iloc[test_idx])))  \n",
        "  return np.mean(maes), np.std(maes)\n",
        "\n",
        "lr_mae, lr_std = cv_error(X, y, lr)\n",
        "print('linear regression: mae=%.3f std=%.3f' % (lr_mae, lr_std))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fze9hi4YCAeJ"
      },
      "outputs": [],
      "source": [
        "class RandomBaseline:\n",
        "  \"\"\"\n",
        "  A random regression baseline that just returns a number\n",
        "  selected uniformly within the range of the max/min values\n",
        "  seen in the training data.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # set our seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.min_y = min(y)\n",
        "    self.max_y = max(y)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.random.uniform(self.min_y, self.max_y, len(X))\n",
        "\n",
        "rb = RandomBaseline()\n",
        "rb.fit(X.head(), y.head())\n",
        "rb.predict(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qw1gNATBv9z"
      },
      "outputs": [],
      "source": [
        "class MeanBaseLine:\n",
        "  \"\"\"\n",
        "  A slightly better baseline that returns the mean of the training set\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.mean = np.mean(y)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return [self.mean]*len(X)\n",
        "\n",
        "me = MeanBaseLine()\n",
        "me.fit(X.head(), y.head())\n",
        "me.predict(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnDHf3puC8l0"
      },
      "outputs": [],
      "source": [
        "rb_mae, rb_std = cv_error(X, y, RandomBaseline())\n",
        "me_mae, me_std = cv_error(X, y, MeanBaseLine())\n",
        "print('random baseline: mae=%.3f std=%.3f' % (rb_mae, rb_std))\n",
        "print('mean baseline: mae=%.3f std=%.3f' % (me_mae, me_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbq3SVwzDRdI"
      },
      "source": [
        "**Whew, at least we're doing better than random**\n",
        "\n",
        "Let's make a convenience function to plot the errors for a list of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Ze0MGnDZen"
      },
      "outputs": [],
      "source": [
        "def compare_errors(errs, stds, labels):\n",
        "  plt.figure()\n",
        "  xs = np.arange(len(errs))\n",
        "  plt.errorbar(xs, errs, fmt='bo', yerr=stds)\n",
        "  plt.xticks(xs, labels)\n",
        "  plt.ylabel('MAE')\n",
        "  plt.show()\n",
        "compare_errors([lr_mae, me_mae, rb_mae], [lr_std, me_std, rb_std], ['linear_regression', 'mean', 'random'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx6kwsa3E5jf"
      },
      "source": [
        "## Polynomial Regression\n",
        "\n",
        "To implement polynomial regression, we'll simply add new feature columns corresponding to higher-order transformations of the input features.\n",
        "\n",
        "We'll implement this ourselves, though\n",
        "sklearn also has a class for this: [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AznxutQpFLDw"
      },
      "outputs": [],
      "source": [
        "def make_polynomial(X, order=2):\n",
        "  \"\"\"\n",
        "  Return a new DataFrame with higher-order transformations\n",
        "  of each column\n",
        "  \"\"\"\n",
        "  X_new = X.copy()\n",
        "  for c in X.columns:\n",
        "    for o in range(2, order+1):\n",
        "      X_new['%s_%d' % (c, o)] = X[c]**o\n",
        "  return X_new\n",
        "\n",
        "display(X.head())\n",
        "X_2 = make_polynomial(X, order=2)\n",
        "X_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70k4-5O_HL7s"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "lr_2_mae, lr_2_std = cv_error(X_2, y, lr)\n",
        "print('quadratic regression: mae=%.3f std=%.3f' % (lr_2_mae, lr_2_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em4TJDkXIT-f"
      },
      "source": [
        "It doesn't look like adding these features helped at all.\n",
        "\n",
        "This is large part due to the small training set. We already have pretty good model capacity with the linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MZioNeGHzxG"
      },
      "outputs": [],
      "source": [
        "# How about training error?\n",
        "mae(lr.fit(X_2, y).predict(X_2), y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2_rgA98IILu"
      },
      "outputs": [],
      "source": [
        "# In linear case, it was...\n",
        "mae(lr.fit(X, y).predict(X), y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fp8VHvpIngC"
      },
      "source": [
        "## Overfitting\n",
        "\n",
        "Let's see how badly we overfit as we consider higher order polynomials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qp-vQh5Is8z"
      },
      "outputs": [],
      "source": [
        "def compare_polynomials(X, y, lr):\n",
        "  train_errs = []\n",
        "  test_errs = []\n",
        "  stds = []\n",
        "  labels = []\n",
        "  for order in range(1,6):\n",
        "    X_i = make_polynomial(X, order=order)\n",
        "    train_errs.append(mae(lr.fit(X_i, y).predict(X_i), y))\n",
        "    lr_i_mae, lr_i_std = cv_error(X_i, y, lr)\n",
        "    test_errs.append(lr_i_mae)\n",
        "    stds.append(lr_i_std)\n",
        "    labels.append('order=%d' % order)\n",
        "  return train_errs, test_errs, stds, labels\n",
        "\n",
        "\n",
        "def plot_overfitting(train_errs, test_errs, stds, labels):\n",
        "  plt.figure()\n",
        "  xs = np.arange(len(test_errs))\n",
        "  plt.errorbar(xs, test_errs, fmt='ro-', yerr=stds, label='Test MAE')\n",
        "  plt.plot(xs, train_errs, 'bo-', label='Train MAE')\n",
        "  plt.xticks(xs, labels)\n",
        "  plt.ylabel('MAE')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "train_errs, test_errs, stds, labels = compare_polynomials(X, y, lr)\n",
        "plot_overfitting(train_errs, test_errs, stds, labels)\n",
        "test_errs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9rVKfkiJoCz"
      },
      "source": [
        "(why does std increase with order??)\n",
        "\n",
        "**So, which model should we choose here??**\n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "This is a disappointing but not uncommon result -- we tried something more complex, and our generalization error did not improve. \n",
        "\n",
        "A big part of machine learning is figuring out new methods that are complex but avoid overfitting.\n",
        "\n",
        "A simple approach is called **L2-regularization**.\n",
        "This adds a penalty term that encourages smaller parameter values.\n",
        "\n",
        "$$\\beta^* \\leftarrow \\text{argmin}_\\beta ~~~ \\text{RSS}(\\beta, X, y) + \\alpha \\sum_k \\beta_k^2$$\n",
        "\n",
        "Here, the RSS error function from before is augmented with the sum of the squares of each parameter. The bigger the $\\beta$s become, the higher our error. \n",
        "\n",
        "The $\\alpha$ parameter trades off the two terms in the objective --- we want to minimize RSS while minimizing weight magnitudes.\n",
        "\n",
        "This approach is implemented in sklearn as [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0oQJGraNALD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "train_errs, test_errs, stds, labels = compare_polynomials(X, y, Ridge(alpha=10))\n",
        "plot_overfitting(train_errs, test_errs, stds, labels)\n",
        "test_errs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyN_4kS5Oq6v"
      },
      "source": [
        "We can get a slightly better error rate with Ridge. Though, the main advantage is that the variation in error is greatly reduced."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}